{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syedfahdali/htmlfrontpage/blob/main/floral_text_generation_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_PdgJ6ak-Dl"
      },
      "source": [
        "# Fine-tuning Stable Diffusion for Floral Text Style using LoRA\n",
        "\n",
        "This notebook outlines the process of fine-tuning a Stable Diffusion model using Low-Rank Adaptation (LoRA) to learn a specific \"floral text\" style from a small dataset of images.\n",
        "\n",
        "The process involves:\n",
        "1. Setting up the environment and installing necessary libraries.\n",
        "2. Loading the base Stable Diffusion model.\n",
        "3. Preparing a dataset from images in a specified folder, using filenames as captions.\n",
        "4. Configuring and running the LoRA training process.\n",
        "5. Saving the trained LoRA weights.\n",
        "6. Evaluating the fine-tuned model by generating images using the base model combined with the LoRA weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write zip extraction code for Images.zip\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def extract_zip(zip_filepath, extract_dir):\n",
        "    \"\"\"Extracts a zip file to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        zip_filepath: Path to the zip file.\n",
        "        extract_dir: Directory to extract the zip file contents to.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Example usage:\n",
        "zip_file_path = './Images.zip'  # Replace with your zip file path\n",
        "extract_directory = 'images' # Replace with your desired extraction directory\n",
        "\n",
        "if not os.path.exists(extract_directory):\n",
        "    os.makedirs(extract_directory)\n",
        "\n",
        "extract_zip(zip_file_path, extract_directory)\n",
        "print(f\"Successfully extracted {zip_file_path} to {extract_directory}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TojmqDJflkQp",
        "outputId": "710190fe-5770-41f5-e27d-e8c24b68d63f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully extracted ./Images.zip to images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n809Dkllk-Dn"
      },
      "source": [
        "## 1. Setup: Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptCp9H6gk-Dn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd38104-93fe-41f2-ef71-8dca66098d5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "# We need diffusers, transformers, accelerate, bitsandbytes (for 8-bit optimizer), peft (for LoRA), and datasets\n",
        "%pip install -q diffusers transformers accelerate bitsandbytes ftfy Pillow # Base dependencies\n",
        "%pip install -q peft datasets # LoRA and dataset handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRnVrk8Zk-Dp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import re\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler, UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from datasets import Dataset as HFDataset # Renamed to avoid conflict with torch Dataset\n",
        "import accelerate\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "from diffusers.optimization import get_scheduler\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from diffusers.utils import make_image_grid\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYbxnAYBk-Dq"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZkORKMek-Dq"
      },
      "outputs": [],
      "source": [
        "# --- Training Configuration ---\n",
        "pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\"\n",
        "revision = None # Use main branch\n",
        "\n",
        "# --- Your Data ---\n",
        "# Assumes images are in ../images relative to this notebook's directory\n",
        "# Corrected path based on user feedback\n",
        "instance_data_dir = \"./images/Images\"\n",
        "output_dir = \"./models/lora_floral_text\" # Where to save LoRA weights\n",
        "\n",
        "# --- LoRA Parameters ---\n",
        "lora_rank = 16 # Rank of the LoRA matrices. Higher rank means more parameters, potentially more expressive but prone to overfitting.\n",
        "lora_alpha = lora_rank # Often set equal to rank\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# --- Training Parameters ---\n",
        "resolution = 512 # Resolution for input images. Must be >= 512.\n",
        "center_crop = True # Whether to center crop the input images to the resolution.\n",
        "train_batch_size = 1 # Batch size (per device) for training. Reduce if OOM.\n",
        "num_train_epochs = 100 # Number of training epochs. Adjust based on results (start with more for small datasets).\n",
        "max_train_steps = None # If set, overrides num_train_epochs.\n",
        "learning_rate = 1e-4 # Initial learning rate.\n",
        "scale_lr = False # Scale learning rate by sqrt(gradient_accumulation_steps * train_batch_size * num_gpus).\n",
        "lr_scheduler_name = \"constant\" # Choose from \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"\n",
        "lr_warmup_steps = 0\n",
        "use_8bit_adam = True # Whether to use 8-bit AdamW optimizer (requires bitsandbytes).\n",
        "adam_beta1 = 0.9\n",
        "adam_beta2 = 0.999\n",
        "adam_weight_decay = 1e-2\n",
        "adam_epsilon = 1e-08\n",
        "max_grad_norm = 1.0 # Max gradient norm for clipping.\n",
        "gradient_accumulation_steps = 1 # Number of updates steps to accumulate before performing a backward/update pass.\n",
        "mixed_precision = \"fp16\" # Choose: \"no\", \"fp16\", \"bf16\".\n",
        "allow_tf32 = True # Allow TF32 on Ampere GPUs for potentially faster training.\n",
        "\n",
        "# --- Other ---\n",
        "seed = 42\n",
        "checkpointing_steps = 200 # Save a checkpoint of the training state every X updates.\n",
        "validation_prompt = \"A floral design with the text 'Hello World'\" # Prompt to use for generating validation images during training\n",
        "num_validation_images = 4\n",
        "validation_epochs = 20 # Run validation every X epochs\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Create a dictionary of hyperparameters for logging\n",
        "config_to_log = {\n",
        "    \"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n",
        "    \"revision\": revision,\n",
        "    \"instance_data_dir\": instance_data_dir,\n",
        "    \"output_dir\": output_dir,\n",
        "    \"lora_rank\": lora_rank,\n",
        "    \"lora_alpha\": lora_alpha,\n",
        "    \"lora_dropout\": lora_dropout,\n",
        "    \"resolution\": resolution,\n",
        "    \"center_crop\": center_crop,\n",
        "    \"train_batch_size\": train_batch_size,\n",
        "    \"num_train_epochs\": num_train_epochs, # Note: This might be recalculated later\n",
        "    \"max_train_steps\": max_train_steps, # Note: This might be recalculated later\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"scale_lr\": scale_lr,\n",
        "    \"lr_scheduler_name\": lr_scheduler_name, # Log the name\n",
        "    \"lr_warmup_steps\": lr_warmup_steps,\n",
        "    \"use_8bit_adam\": use_8bit_adam,\n",
        "    \"adam_beta1\": adam_beta1,\n",
        "    \"adam_beta2\": adam_beta2,\n",
        "    \"adam_weight_decay\": adam_weight_decay,\n",
        "    \"adam_epsilon\": adam_epsilon,\n",
        "    \"max_grad_norm\": max_grad_norm,\n",
        "    \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "    \"mixed_precision\": mixed_precision,\n",
        "    \"allow_tf32\": allow_tf32,\n",
        "    \"seed\": seed,\n",
        "    # Add validation params too for completeness\n",
        "    \"validation_prompt\": validation_prompt,\n",
        "    \"num_validation_images\": num_validation_images,\n",
        "    \"validation_epochs\": validation_epochs,\n",
        "    \"checkpointing_steps\": checkpointing_steps,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOPt08Skk-Dr"
      },
      "source": [
        "## 3. Load Models and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLY4sUrsk-Dr"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    subfolder=\"tokenizer\",\n",
        "    revision=revision,\n",
        ")\n",
        "\n",
        "# Load the text encoder\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    subfolder=\"text_encoder\",\n",
        "    revision=revision,\n",
        ")\n",
        "\n",
        "# Load the VAE\n",
        "# We use the VAE from the original model, no fine-tuning needed here\n",
        "vae = AutoencoderKL.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    subfolder=\"vae\",\n",
        "    revision=revision,\n",
        ")\n",
        "\n",
        "# Load the UNet model\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    subfolder=\"unet\",\n",
        "    revision=revision,\n",
        ")\n",
        "\n",
        "# Freeze VAE and text_encoder parameters\n",
        "vae.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)\n",
        "\n",
        "# Set UNet parameters to be trainable (initially)\n",
        "unet.train()\n",
        "\n",
        "# Move models to device\n",
        "vae.to(device, dtype=torch.float16) # VAE often works well in float16\n",
        "text_encoder.to(device)\n",
        "unet.to(device)\n",
        "\n",
        "print(\"Models loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InjdYShKk-Dr"
      },
      "source": [
        "## 4. Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl5rKAxPk-Ds"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, data_dir, tokenizer, size=512, center_crop=True):\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "\n",
        "        self.image_paths = [p for p in self.data_dir.iterdir() if p.is_file() and p.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']]\n",
        "        print(f\"Found {len(self.image_paths)} images in {data_dir}\")\n",
        "\n",
        "        self.image_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5], [0.5]), # Normalize to [-1, 1]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        # Derive caption from filename (remove extension, replace underscores/hyphens with spaces)\n",
        "        caption = img_path.stem.replace('_', ' ').replace('-', ' ')\n",
        "        # Simple cleanup: remove extra spaces\n",
        "        caption = re.sub(r'\\s+', ' ', caption).strip()\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            pixel_values = self.image_transforms(image)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing image {img_path}: {e}\")\n",
        "            # Return dummy data or skip? For simplicity, returning None here.\n",
        "            # A real implementation might handle this more robustly.\n",
        "            return None\n",
        "\n",
        "        # Tokenize caption\n",
        "        input_ids = self.tokenizer(\n",
        "            caption, padding=\"max_length\", truncation=True, max_length=tokenizer.model_max_length, return_tensors=\"pt\"\n",
        "        ).input_ids\n",
        "\n",
        "        return {\"pixel_values\": pixel_values, \"input_ids\": input_ids.squeeze(0)} # Remove batch dim from input_ids\n",
        "\n",
        "# Create the dataset\n",
        "train_dataset = ImageCaptionDataset(\n",
        "    instance_data_dir,\n",
        "    tokenizer,\n",
        "    size=resolution,\n",
        "    center_crop=center_crop\n",
        ")\n",
        "\n",
        "# Collate function to handle potential None values from dataset errors\n",
        "def collate_fn(examples):\n",
        "    examples = [e for e in examples if e is not None] # Filter out None values\n",
        "    if not examples:\n",
        "        return None # Or raise an error if no valid data in batch\n",
        "\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
        "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
        "\n",
        "# Create DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Dataset size: {len(train_dataset)}\")\n",
        "print(f\"Dataloader created. Batch size: {train_batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2GsLK4lk-Ds"
      },
      "source": [
        "## 5. Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbwA0Vr6k-Dt"
      },
      "outputs": [],
      "source": [
        "# Configure LoRA for the UNet\n",
        "lora_config = LoraConfig(\n",
        "    r=lora_rank,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"], # Common target modules for SD LoRA\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\", # Usually set to none for LoRA\n",
        ")\n",
        "\n",
        "# Add LoRA adapters to the UNet model\n",
        "unet = get_peft_model(unet, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "unet.print_trainable_parameters()\n",
        "\n",
        "print(\"LoRA configured for UNet.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEBh0bTZk-Dt"
      },
      "source": [
        "## 6. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCsJ8dP3k-Dt"
      },
      "outputs": [],
      "source": [
        "# Set up accelerator\n",
        "accelerator = accelerate.Accelerator(\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    mixed_precision=mixed_precision,\n",
        "    log_with=\"tensorboard\",\n",
        "    project_dir=os.path.join(output_dir, \"logs\")\n",
        ")\n",
        "\n",
        "# Use 8-bit AdamW if enabled\n",
        "if use_8bit_adam:\n",
        "    try:\n",
        "        import bitsandbytes as bnb\n",
        "    except ImportError:\n",
        "        raise ImportError(\"Please install bitsandbytes to use 8-bit AdamW. `pip install bitsandbytes`\")\n",
        "    optimizer_cls = bnb.optim.AdamW8bit\n",
        "else:\n",
        "    optimizer_cls = torch.optim.AdamW\n",
        "\n",
        "# Optimizer targets only the LoRA parameters in the UNet\n",
        "optimizer = optimizer_cls(\n",
        "    unet.parameters(), # Only optimize UNet LoRA parameters\n",
        "    lr=learning_rate,\n",
        "    betas=(adam_beta1, adam_beta2),\n",
        "    weight_decay=adam_weight_decay,\n",
        "    eps=adam_epsilon,\n",
        ")\n",
        "\n",
        "# Calculate total training steps\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "if max_train_steps is None:\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "else:\n",
        "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "# Learning rate scheduler\n",
        "# Explicitly fetch the scheduler name from the config dictionary\n",
        "current_scheduler_name = config_to_log['lr_scheduler_name']\n",
        "lr_scheduler_obj = get_scheduler(\n",
        "    current_scheduler_name, # Pass the fetched name string\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps,\n",
        "    num_training_steps=max_train_steps * gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "# Prepare everything with accelerator\n",
        "unet, optimizer, train_dataloader, lr_scheduler_obj = accelerator.prepare(\n",
        "    unet, optimizer, train_dataloader, lr_scheduler_obj # Pass the scheduler object\n",
        ")\n",
        "\n",
        "# We need to recalculate our total training steps as the size of the dataloader may have changed.\n",
        "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
        "if max_train_steps is None:\n",
        "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "else:\n",
        "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
        "\n",
        "# Move text_encoder and vae to GPU (if not already there) - they aren't prepared by accelerator\n",
        "text_encoder.to(accelerator.device)\n",
        "vae.to(accelerator.device)\n",
        "\n",
        "# Keep vae and text_encoder in eval model as we don't train them\n",
        "vae.eval()\n",
        "text_encoder.eval()\n",
        "\n",
        "# We need to initialize the trackers we use, and also store our configuration.\n",
        "# The trackers initializes automatically on the main process.\n",
        "if accelerator.is_main_process:\n",
        "    # Use the specific config dictionary for logging\n",
        "    accelerator.init_trackers(\"lora_floral_text\", config=config_to_log)\n",
        "\n",
        "print(f\"***** Running training *****\")\n",
        "print(f\"  Num examples = {len(train_dataset)}\")\n",
        "print(f\"  Num Epochs = {num_train_epochs}\")\n",
        "print(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
        "print(f\"  Total train batch size (w. parallel, distributed & accumulation) = {train_batch_size * accelerator.num_processes * gradient_accumulation_steps}\")\n",
        "print(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
        "print(f\"  Total optimization steps = {max_train_steps}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o3hBdANk-Dt"
      },
      "source": [
        "## 7. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I18u2Jtk-Du"
      },
      "outputs": [],
      "source": [
        "global_step = 0\n",
        "first_epoch = 0\n",
        "\n",
        "# Load noise scheduler\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
        "\n",
        "progress_bar = tqdm(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)\n",
        "progress_bar.set_description(\"Steps\")\n",
        "\n",
        "for epoch in range(first_epoch, num_train_epochs):\n",
        "    unet.train()\n",
        "    train_loss = 0.0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if batch is None:\n",
        "            print(f\"Skipping step {step} due to batch error.\")\n",
        "            continue\n",
        "\n",
        "        with accelerator.accumulate(unet):\n",
        "            # Convert images to latent space\n",
        "            latents = vae.encode(batch[\"pixel_values\"].to(dtype=torch.float16)).latent_dist.sample()\n",
        "            latents = latents * vae.config.scaling_factor\n",
        "\n",
        "            # Sample noise that we'll add to the latents\n",
        "            noise = torch.randn_like(latents)\n",
        "            bsz = latents.shape[0]\n",
        "\n",
        "            # Sample a random timestep for each image\n",
        "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
        "            timesteps = timesteps.long()\n",
        "\n",
        "            # Add noise to the latents\n",
        "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            # Get text embedding for conditioning\n",
        "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
        "\n",
        "            # Define loss target\n",
        "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
        "                target = noise\n",
        "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
        "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
        "\n",
        "            # Model prediction and loss\n",
        "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
        "\n",
        "            avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n",
        "            train_loss += avg_loss.item() / gradient_accumulation_steps\n",
        "\n",
        "            # Backward\n",
        "            accelerator.backward(loss)\n",
        "            if accelerator.sync_gradients:\n",
        "                params_to_clip = unet.parameters()\n",
        "                accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)\n",
        "            optimizer.step()\n",
        "            lr_scheduler_obj.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "            accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
        "            train_loss = 0.0\n",
        "\n",
        "            if global_step % checkpointing_steps == 0:\n",
        "                if accelerator.is_main_process:\n",
        "                    save_path = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n",
        "                    accelerator.save_state(save_path)\n",
        "                    print(f\"Saved state to {save_path}\")\n",
        "\n",
        "        logs = {\"step_loss\": loss.detach().item(), \"lr\": lr_scheduler_obj.get_last_lr()[0]}\n",
        "        progress_bar.set_postfix(**logs)\n",
        "\n",
        "        if global_step >= max_train_steps:\n",
        "            break\n",
        "\n",
        "    # Validation loop\n",
        "    if accelerator.is_main_process:\n",
        "        if epoch % validation_epochs == 0 or epoch == num_train_epochs - 1:\n",
        "            print(f\"\\nRunning validation... Epoch {epoch}\")\n",
        "            unet_inference = accelerator.unwrap_model(unet)\n",
        "            pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "                pretrained_model_name_or_path,\n",
        "                unet=unet_inference,\n",
        "                text_encoder=text_encoder,\n",
        "                vae=vae,\n",
        "                revision=revision,\n",
        "                torch_dtype=torch.float16,\n",
        "            )\n",
        "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
        "            pipeline = pipeline.to(accelerator.device)\n",
        "            pipeline.set_progress_bar_config(disable=True)\n",
        "\n",
        "            generator = torch.Generator(device=accelerator.device).manual_seed(seed) if seed else None\n",
        "            images = []\n",
        "            for _ in range(num_validation_images):\n",
        "                with torch.autocast(\"cuda\"):\n",
        "                    image = pipeline(validation_prompt, num_inference_steps=25, generator=generator).images[0]\n",
        "                images.append(image)\n",
        "\n",
        "            # Create image grid and log it\n",
        "            grid = make_image_grid(images, rows=1, cols=num_validation_images)\n",
        "\n",
        "            # ✅ FIXED image logging\n",
        "            from torchvision import transforms\n",
        "            writer = accelerator.get_tracker(\"tensorboard\").writer\n",
        "            writer.add_image(\"validation\", transforms.ToTensor()(grid), epoch)\n",
        "\n",
        "            # Save grid locally\n",
        "            grid.save(os.path.join(output_dir, f\"validation_epoch_{epoch}.png\"))\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# End training\n",
        "accelerator.wait_for_everyone()\n",
        "\n",
        "# Save LoRA weights\n",
        "if accelerator.is_main_process:\n",
        "    unet = accelerator.unwrap_model(unet)\n",
        "    unet.save_pretrained(output_dir)\n",
        "    print(f\"LoRA weights saved to {output_dir}\")\n",
        "\n",
        "accelerator.end_training()\n",
        "print(\"Training finished.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svcHCykSk-Du"
      },
      "source": [
        "## 8. Evaluation / Example Generation with LoRA\n",
        "\n",
        "Load the base model and attach the trained LoRA weights to generate images."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DConditionModel\n",
        "unet = accelerator.unwrap_model(unet)\n",
        "UNet2DConditionModel.save_attn_procs(unet, output_dir)\n",
        "print(f\"✅ LoRA weights saved correctly to {output_dir}\")\n"
      ],
      "metadata": {
        "id": "o4PF3O0BBOdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import re\n",
        "from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n",
        "\n",
        "# --- Setup ---\n",
        "pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
        "output_dir = \"./models/lora_floral_text\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "seed = 42\n",
        "\n",
        "# --- Load Base ---\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    pretrained_model_name_or_path,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(device)\n",
        "\n",
        "# --- Load LoRA Adapter with specific weight file ---\n",
        "# Replace with the exact name of the LoRA weight file if different\n",
        "lora_weight_path = f\"{output_dir}/pytorch_lora_weights.bin\"\n",
        "print(f\"Loading LoRA adapter from: {lora_weight_path}\")\n",
        "pipe.load_lora_weights(output_dir, weight_name=\"pytorch_lora_weights.bin\")\n",
        "print(\"LoRA adapter loaded ✅\")\n",
        "\n",
        "# --- Prompts ---\n",
        "test_prompts = [\n",
        "    'A floral design with the text \"Welcome\" in elegant script',\n",
        "    'The word \"Love\" made of pink roses and green vines',\n",
        "    '\"Shine\" text with sunflowers and bright yellow petals',\n",
        "    '\"Dream\" written in cursive with lavender flowers'\n",
        "]\n",
        "\n",
        "num_inference_steps = 30\n",
        "guidance_scale = 7.5\n",
        "\n",
        "# --- Generate Images ---\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\n--- Generating: {prompt} ---\")\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "\n",
        "    with torch.autocast(device):\n",
        "        image = pipe(\n",
        "            prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            generator=generator\n",
        "        ).images[0]\n",
        "\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "        display(image)\n",
        "    except ImportError:\n",
        "        safe_prompt = re.sub(r'[^a-zA-Z0-9_]+', '_', prompt)[:50]\n",
        "        image.save(f\"{safe_prompt}_lora_output.png\")\n",
        "        print(f\"Saved to {safe_prompt}_lora_output.png\")\n"
      ],
      "metadata": {
        "id": "V072eIovEz_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show diffusers"
      ],
      "metadata": {
        "id": "S0rXkG7UE6e6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9we7pHd_Bst3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}